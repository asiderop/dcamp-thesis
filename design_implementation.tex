\chapter{Design \& Implementation}
\label{design_implementation}

\textbf{General design:} semi-centralized, hierarchical peer-to-peer system utilizing the pipe-flow architecture pattern
in which leaf (sensor) nodes of the hierarchy collect data, filter out extraneous data, and send it up the pipe to an
aggregate node which subsequently filters out more data and sends it up to another aggregate or root node.

\section{\dcamp Roles and Services}

The dCAMP distributed system is comprised of one or more nodes each running one or more roles. Each role--a published,
remotely accessible interface--provides one or more sets of functionality. Each set of functionality is known as a
service.

\subsection{Services}

\begin{itemize}

\item \textbf{Node}---rudimentary dCAMP functionality; handles topology communication, heartbeat monitoring, and failure
recovery.

\item \textbf{Sensor}---local performance metric gathering; essentially the dCAMP layer on top of the OS and hardware
performance APIs (accessed via CAMP).

\item \textbf{Filter}---performance metric filtering; provides throttling and thresholding of metrics.

\item \textbf{Aggregation}--—performance metric aggregation; provides collection of and calculation on metrics from
multiple sensors and/or collectors.

\item \textbf{Management}--—primary entry-point for end-user control of dCAMP distributed system; this is the dCAMP
instrument panel, providing basic administration functions (e.g. start, stop, etc.).

\item \textbf{Configuration}--—complete configuration replication; provides topology and configuration distribution.

\end{itemize}

\subsection{Roles}

The \textit{Base} role must be running on each node for it to be part of the dCAMP distributed system. In this document,
a "Base node" is defined as a dCAMP node which has not yet been configured, i.e. it has not joined a running dCAMP
system.

The \textit{Metric} role runs on the nodes from which performance metrics should be collected. The \textit{Collector}
role acts as an aggregation point in the system, combining performance data from multiple \textit{Metric} (and
\textit{Collector}) nodes and providing additional aggregated performance metrics.

There is only one \textit{Root} role active in the system; it acts as the master copy of the dCAMP configuration and
sole user-interface point. The \textit{Root} role is not strictly attached to any given node in the system. Rather, the
\textit{Root} role may dynamically move to any first-level \textit{Collector} node if the current \textit{Root} node
fails.

Depending on the use case and desired system performance, an administrator may choose to split roles across multiple
nodes or collapse them onto a single node. For example, a single node may act as \textit{Metric}, \textit{Collector},
and \textit{Root} for smaller systems while larger systems would employ dedicated \textit{Collector} nodes.

\subsection{Role-to-Service Mapping}

The following table lists the roles which can be "published" by a dCAMP node and the services which they implement.

\begin{tabular}{|l|l|}

\hline
\textbf{Role} & \textbf{Service(s)} \\
\hline
Root & Management, Aggregation, Filter, Configuration \\
\hline
Collector & Aggregation, Filter, Configuration \\
\hline
Metric & Sensor, Filter, Configuration \\
\hline
Base & Node \\
\hline

\end{tabular}

\section{Requirements}

\subsection{Functional Requirements}

\begin{enumerate}

\item Configuration
  \begin{itemize}
  \item instantiation, administration
  \item topology coordination
  \item metric collection spec.
  \end{itemize}

\item Metric Collection
  \begin{itemize}
  \item dCAMP API on top of CAMP
  \item filters (at various levels), thresholds
  \item aggregation of metrics across nodes
  \item output to log file
  \end{itemize}

\item Fault Tolerance
  \begin{itemize}
  \item simple rules to handle failures
  \end{itemize}

\end{enumerate}

\subsection{Fault Tolerance}

\begin{enumerate}

\item Topology MUST sustain brief network disconnectivity of any node.

\item Sensor nodes MUST be allowed to enter/exit topology at any time.

\item Root/Collector nodes (i.e. parents) MUST failover in case of extended disconnectivity.
  \begin{itemize}
  \item Loss of previously collected data SHOULD be minimized during failover.
  \end{itemize}

\item Management node SHOULD be allowed to enter/exit topology at any time.

\end{enumerate}

\subsection{Testing}

\begin{itemize}

\item \textbf{Transparency:} dCAMP SHOULD introduce negligible performance impact on sensor nodes.

\item \textbf{Accuracy:} dCAMP MUST accurately report metrics/performance of sensor nodes (individual and aggregated).

\item \textbf{Scalability:} dCAMP SHOULD maintain its transparency and accuracy as it scales (i.e. the number of sensor nodes
increases).

\item \textbf{Fault Tolerance:} dCAMP MUST successfully handle entrance/exit of any node(s) in the system.

\end{itemize}

\section{Execution Steps}

These steps are for the current, prototype version of \dcamp. \textit{Setup: All nodes are running as base nodes
(listening for commands via CLI and network port)}

\begin{enumerate}

\item User initializes root node via CLI (providing configuration) 
\item Root node registers each listening node (configuring as metric nodes) 
\item Metrics are sampled, reported, and logged (repeating indefinitely) 
\item Metric nodes sample metrics and report metrics back to root node 
\item Root node logs received metrics to disk 
\item User shuts down root node via CLI 
\item Root node unregisters each listening node (reverting to base nodes) 
\item Root node terminates (reverting to base node)

\end{enumerate}

\section{ZeroMQ Protocols}

For a quick background on ZeroMQ socket types and message patterns, please see \hyperref[zeromq_primer]{ZeroMQ Primer}.

\input{proto_topo}
\input{proto_conf}
\input{proto_data}
% \input{proto_elec}
