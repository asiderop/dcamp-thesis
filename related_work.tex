\chapter{Related Work}
\label{related_work}

\emph{THIS SECTION IS FROM MY 508 PAPER \newline AND NEEDS TO BE CLEANED UP}

Being distributed, a framework must collect data from a large number of nodes and aggregate the data to one node or client.
Implementations have been built using centralized, hierarchical, peer-to-peer and any number of other architectures.
There are three types of metric gathering techniques: (1) hardware counters and sensors use specialized hardware to gather highly accurate metrics and are highly dependent on the underlying hardware architecture, (2) software sensors use modern operating system interfaces to acquire moderately accurate performance metrics in an architecture-independent interface, and (3) hybrid approaches use a combination of hardware and software sensors to attain a balance between the two.

There are a number of distributed performance frameworks being actively researched and developed, both academically and commercially.
This work defines a set of criteria for evaluating distributed performance frameworks to measure the usefulness and viability of the framework.
In the next section I list similar survey work, showing the novelty of this paper.
Section 3 presents the evaluation criteria, section 4 provides the evaluation results of several distributed performance frameworks, and section 5 concludes.

\section{Related Work}
Several papers have been published dealing with this topic and which try to give an overview of the current state of distributed performance frameworks.
This work, while related, differs in two main aspects: (1) the evaluation criteria presented here is more complete than previous works' requirements and (2) this work looks specifically at distributed performance frameworks rather than distributed system or grid tools and frameworks.

Zoraja et al. include a section on Monitoring Issues in their work on \newline OMIS/OCM, CORAL, and MIMO \cite{zoraja1999} focusing on design and implementation issues for monitoring systems.
These monitoring issues share some critical points with the criteria presented here, but the work lacks an analysis of other distributed performance frameworks and only includes limited analysis of the presented frameworks against the listed monitoring issues.
\cite{hofmann1994} provides a short survey of related monitoring systems in comparison to the ZM4/SIMPLE monitoring environment.
While this survey does look at a number of important framework characteristics, it is restricted to hybrid distributed performance frameworks.

Allan and Ashworth published an exhaustive survey of distributed computing tools \cite{allan2001} but did not present any particular focus on distributed performance frameworks nor did they include a set of criteria for evaluating frameworks.
Zanikolas et al. present an in-depth taxonomy of grid monitoring systems \cite{zanikolas2005}, including a set of general requirements for monitoring systems.
Their work does much for defining a good set of criteria for evaluating and analyzing distributed performance frameworks, but has a much broader scope than this work.
This work expands on their monitoring system requirements to account for the completeness and validity of distributed performance frameworks.

\section{Analysis Results}
The distributed performance frameworks analyzed in this section were chosen based on their categorization in the \cite{zanikolas2005} taxonomy; only level 2 frameworks are included.
Level 2 frameworks are defined as having at least one type of republisher in addition to producers; these frameworks usually distribute functionality across multiple hosts. \cite{zanikolas2005}
A limited analysis is conducted by reviewing the available literature, and further analysis (i.e., verifying scalability, transparency, and validity) is left as future work.

\subsection{NetLogger}
Work done by Brian Tierney and Dan Gunter \cite{tierney1998} \cite{gunter2000} presents the Network Application Logger Toolkit (NetLogger).
This framework can be used to monitor the performance of distributed systems at a very detailed level.
With a new logging format and activation service \cite{gunter2002} the authors improved upon their previous work and increased the toolkit's scalability and data delivery models.
NetLogger is being actively developed and is one of the more well known distributed performance frameworks.
The toolkit is composed of four parts: an API and library for instrumenting a given application, a set of tools for collecting and sorting logs, performance sensors, and a visualization user interface for the log files.

Each part assumes the system clocks of the individual nodes are accurate and synchronized (the authors mention the use of NTP to achieve a required clock synchronization of one millisecond).
The instrumentation of code allows NetLogger to gather more detailed data from an application-to-application communication path, such as traces of network packets through a call hierarchy.
The instrumentation also allows the activation service to update the monitoring of parts of the system dynamically as consumers subscribe to various events and metrics.

Their research has shown NetLogger to be highly scalable, complete, and transparent as well as valid.
The activation service provides a push data delivery and can utilize the security mechanisms part of current web services in order to authenticate requests for performance data.
NetLogger is currently implemented for C, C++, Java, Perl, and Python applications.
Because the framework lacks black box characteristics, its portability is greatly reduced.

\subsection{JAMM}
Java Agents for Monitoring and Management (JAMM) \cite{tierney2000} is the fruit of work by the authors of NetLogger to build a monitoring system with managed sensors. 

The JAMM system consists of six components: sensors, sensor managers, event gateways, directory service, event consumers, and event archives.
There is a sensor manager on each host, with the sensors acting as producers for the gateways which they publish the data to.
The gateways can then filter and aggregate the incoming data according to consumer queries.
The directory service is used to publish the location of the sensors and gateways, allowing for dynamic discovery of active sensors by the consumers.
The event archive is used for historical analysis purposes.

JAMM explicitly uses a pull data delivery model where data is only sent when requested by a consumer.
The overall architecture is generally distributed with the directory service being centralized.
JAMM, being heavily based off of NetLogger, inherits the validity, completeness, security, and transparency of NetLogger along with its lack of portability.
JAMM does, however, prove itself in terms of scalability with it's own architecture.

\subsection{Hawkeye}
Hawkeye \cite{hawkeye} is a monitoring and management tool for distributed systems which makes use of technology previously researched and developed as part of the Condor project \cite{litzkow1988}.
Condor provides mechanisms for collecting information about large distributed computer systems.
Hawkeye is being readily developed and is freely available for download on Linux and Solaris.

Hawkeye uses a general push delivery model by configuring Condor to execute programs, or modules, at given time intervals, collect performance data, and send it to the central manager.
These modules are configurable such that the "period' of module execution can be set to a given time frame in seconds, minutes, or hours or the module can be executed in "continuous" mode where the module's execution never ends.
The available modules for monitoring a Condor pool include: disk space, memory used, network errors, open files, CPU monitoring, system load, users, Condor Node, Condor Pool, and Grid Probe.
Custom modules can also be developed and installed for monitoring of arbitrary resources and metrics.
Data can be accessed from the central manager via an API, CLI, or GUI.

While no experiments have been run, the generally centralized manager reduces the Hawkeye framework's scalability, and its transparency is unknown.
The frameworks module based producer architecture gives it an infinite completeness, but being only available on Linux and Solaris makes the framework less portable.
Lastly, the ability to run jobs securely on target machines has been left as future work by the authors.

\subsection{SCALEA-G}
Truong and Fahringer present SCALEA-G \cite{truong2004}, an unified monitoring and performance analysis system for distributed systems.
It is based on the Open Grid Service Architecture \cite{foster2002} and allows for a number of services to monitor both grid resources and grid application.
SCALEA-G uses dynamic instrumentation to profile and trace Java and C/C++ applications in both push and pull data delivery models, making the framework both scalable and portable.

The SCALEA-G framework is composed of several services: directory service, archival service, sensor manager service, instrumentation service, client service, and user portal.
These services provide the following functionality respectively: publishing and searching of producers and consumers, storage of performance results, management of sensors, dynamic instrumentation of source code, administering clients and analyzing data, and on-line monitoring and performance analysis.

The framework makes use of secure sockets to achieve secure communications and achieves high completeness via code instrumentation.
Unfortunately, the authors do not provide any report on SCALEA-G's validity or transparency.

\subsection{IMPuLSE}
Integrated Monitoring and Profiling for Large Scale Environments \cite{bridges2004} was designed to address "operating system-induced performance anomalies" and provide "accurate, low-overhead, whole-system monitoring." The authors have chosen to develop a message-centric approach which associates data with messages rather than hosts and a system-wide statistical sampling to increases the framework's scalability.

The IMPuLSE framework is still in the design stage, and therefore lacks any implementation data outside of their new message-centric design pattern which shows promising results.
Unfortunately, this leaves the framework with unknown transparency, security, completeness, portability, and validity.

\section{Conclusion}
There are a number of high quality and effective distributed performance frameworks being actively researched and developed, but with some frameworks having more research than others, there is a natural disparity of information about each framework.
While the frameworks vary in distributed architecture and features, they all fulfill the minimum requirements of performance frameworks.
The frameworks analyzed in this work are mainly software based sensor frameworks.
This was chosen due to the inherent portability advantage of software sensors over hardware or hybrid sensors.

Many authors have failed to address their framework's validity, transparency, and scalability explicitly, thinking the framework's architecture speaks for itself or blindly assuming it is accurate and introduces negligible load on the measured system.
I leave it as future work to conduct formal experiments to test validity, transparency, and scalability of the distributed performance frameworks analyzed here.

