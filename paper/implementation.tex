\chapter{Implementation}
\label{implementation}

\section{\dcamp Operation}

\subsection{Sequence of \dcamp Operation}
\label{operation_sequnce}

The following steps describe how the \dcamp system is turned on. The \textit{Base} nodes (other than the node assigned
to be the \textit{Root}) can be started at any time by using the \dcamp CLI, before or after the \textit{Root} node is
initialized. It is expected these \textit{Base} nodes are managed by a watchdog utility which automatically restarts the
node if it exits for any reason.

\begin{figure}[H]
\vspace{+10pt}
\begin{lstlisting}[language=bash,frame=single,basicstyle=\footnotesize\ttfamily]
#!/usr/bin/env bash
while [ true ]
do
    dcamp base --address localhost:56789
done
\end{lstlisting}
\vspace{-10pt}
\caption{Sample Watchdog Script}
\label{fig:sample_watchdog}
\end{figure}

\begin{enumerate}

\item User promotes a \textit{Root} node via the \dcamp CLI, specifying a configuration file and a \textit{Base} node's
      address.
\item \textit{Root} node connects to each \textit{Base} node and begins the ``discover'' \hyperref[proto_topo]{Topology
      Protocol}.
\item \textit{Base} nodes join the \dcamp system at any time, being assigned as \textit{Collector} or \textit{Metric}
      nodes in the topology.

\item \dcamp runs in a steady state, nodes entering or exiting the system at any time.

      \begin{itemize}
      \item Performance counters are sampled, filtered, reported, and logged by the Metric nodes at regular intervals
            according to the \hyperref[configuration]{\dcamp Configuration}.
      \item Performance counters received from child nodes are aggregated, filtered, reported, and logged by
            \textit{Collector} nodes at regular intervals according to the \dcamp Configuration.
      \item Performance counters received from child nodes are aggregated and logged by \textit{Root} node for later
	    processing (e.g. graphing metrics during a test scenario or correlating statistics with a distributed event
	    log).
      \end{itemize}

\item User stops \dcamp by using the \dcamp CLI command.
\item \textit{Root} node begins the ``stop'' Topology Protocol.
\item \textit{Collector} and \textit{Metric} nodes exit the topology and revert to \textit{Base} nodes.
\item \textit{Root} node exits, reverting to \textit{Base} node.

\end{enumerate}

\subsection{Threading Model}
\label{threading_model}

As mentioned above as the first and third steps of \dcamp operation, a \textit{Base} node can transform into one of the
three active \dcamp roles: \textit{Root}, \textit{Collector}, or \textit{Metric}. This transformation is actually the
\textit{Base} role (via the Node service) launching and managing another role internally. This interaction is depicted
in Figure \ref{fig:node_role_service_image}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{node-role-service.pdf}
    \caption[Node, Role, Services Threading Model Diagram]
            {Node, Role, Services Threading Model Diagram: Thread boundaries are represented by dashed lines. Except for
	     the Node service's \texttt{SUB} and \texttt{REQ} sockets, all arrows represent \texttt{PAIR} socket
	     communication.}
    \label{fig:node_role_service_image}
\end{figure}

When a \textit{Base} node is running, only the bottom two threads (the \textit{Base} role and the Node service) are
active. Once it receives an assignment from the ``discover'' Topology Protocol or the \dcamp CLI, the Node service
launches an appropriate role thread which, in turn, launches one or more role-specific service threads.

All communication between the roles and services occurs across \texttt{PAIR} control sockets. There are also various
service-to-service communications which occur via \texttt{inproc} transport sockets (e.g. the internal
\hyperref[proto_data]{Data Flow Protocol}) and shared memory data structures (e.g. the Configuration service).

Also mentioned in section \ref{operation_sequnce} as the last two steps, each role exits and, by doing so, reverts
itself back to a \textit{Base} node. This is handled just like before, with the Node service receiving a \texttt{STOP}
message via the ``stop'' Topology Protocol and then notifying the internally running role to shut down. The role thread
then notifies its service threads, waits for them to finish, then exits.

\section{ZeroMQ Protocols}

ZeroMQ is a fantastic message queuing framework that essentially provides more intelligent sockets as building blocks
for distributed systems. ZeroMQ handles the intricacies of sending messages between two endpoints and lets the
application handle the rest of the logic. The protocols described in this section do not come \textit{from} ZeroMQ,
rather they are built \textit{using} ZeroMQ sockets and message patters.

For a quick background on ZeroMQ socket types and message patterns, please see Appendix \ref{zeromq_primer}.

\input{paper/proto_topo}
\input{paper/proto_conf}
\input{paper/proto_data}
\input{paper/proto_reco}

