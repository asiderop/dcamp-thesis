\chapter{Future Work}
\label{future_work}

\subsubsection{Additional Features}

These features would provide a more complete, end-to-end solution for distributed performance monitoring.

\begin{itemize}

\item end-to-end tool built on top of \dcamp; aggregation provides benefit of looking at large parts of system quickly;
you can look at the aggregate and drill down to find the problem nodes

\item Lightweight web server on each node to support REST APIs and graphical interface.

\item security --- send salted passphrase with every control message, encrypt all messages, etc.

\item provide full support for metric sampling in collector role (instead of requiring two nodes on single machine)

\end{itemize}

\subsubsection{Fault Tolerance}

Improve the fault tolerance of \dcamp by implementing these features which were out-of-scope for the original project.

\begin{itemize}

\item network failure: \dcamp does not support any fault tolerance for network failures; \dcamp only attempts to recover
from node failures. It is assumed that if (part of) the network goes down, the lack of data from that subnet will
suffice. specifically, \dcamp does not tolerate the split brain syndrome\cite{needed}.

\item time accuracy: The system time among multiple nodes in the system may vary significantly; \dcamp is not meant to
be a high-resolution system with respect to the order of performance data occurrences. It is assumed that NTP provides
sufficient time synchronization across all nodes in the system OR the precise ordering of performance events in the
system is not required.

\item self-restarting executable for base node

\item detect collector failure in the root role, not just metric role

\item Root node does not need to be present for the system to operate. 1) Root (i.e. `Management` service) node can come
and go, 2) all data is stored at top-level Collector nodes, 3) Root comes online and asks Collectors for data (since
T?), 4) there is a ``master'' Collector node for config (is that same as root?)

\end{itemize}

\subsubsection{Improve Performance and Scalability}

Increase efficiency of protocols and make really large \dcamp systems possible.

\begin{itemize}

\item use IOLoop (green events?) instead of polling sockets for messages
      \begin{itemize}
      \item look into using an io loop with callbacks to better manage sockets
      \item possibly, use a single io loop, hosted by the base node service and passed to the roles it starts?
      \item this could greatly increase the scalability of dCAMP and reduce its overhead
      \item except the sensor service, most threads sit idly waiting for messages to arrive; this is a waste of system
            resources; using events reduces complexity of implementation and improves runtime performance
      \end{itemize}

\item multicast topo discovery -- reduce config cost, use gateways to multicast into other subnets

\item multiple-level branches are not supported in the current implementation. that is, all collector nodes have the
      root node as their parent and only have leaf nodes as their children.
      \begin{itemize}
      \item support would allow large group configurations to be automatically split into multiple (identically
            configured) branches for improved scalability
      \end{itemize}

\end{itemize}

\subsubsection{Metric Extensions}

Only a small subset of metrics were implemented in \dcamp as a proof of concept. The rest of the full set listed in
\hyperref[dcamp_metrics]{\dcamp Metrics} section are left as future work.

\textbf{VARIABLE LENGTH DATA}
Does \dcamp need to support arbitrary data lengths?

\textbf{NESTED METRICS}
It could be possible for data messages to contain nested data messages, e.g. average/sum of rates.

\textbf{GROUPINGS}
\dcamp may need a more compact data message format for combining multiple metrics into a single message, e.g. for
aggregation purposes or representing entire branches in the topology.
group by source/type/time?
group start/end frames?

\textbf{HISTOGRAMS}
\dcamp should provide a metric histogram ability. Perhaps this should done at each node or only at the root.

\textbf{METRIC REQUESTS}
The admin can use \dcamp to request metrics on a one-time basis, for example, to enumerate the available disks on each
node. The key here is a metric collection is not based on configuration file but rather on real-time input from the
end-user.

Perhaps there should be some special, e.g. ``once'', metric collection specifications so config data can be sent up to the
root only at node start.

