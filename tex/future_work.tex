\subsubsection{Additional Features}

While \dcamp in its current implementation meets the requirements of a basic DPF, these features should advance it into
a more complete, end-to-end distributed performance monitoring solution.

An \textbf{end-to-end tool} built on top of \dcamp could allow a system administrator to quickly look at the performance
of a large part of the network via aggregate metrics and easily drill down into the groups and/or nodes which exhibit
problematic behaviour. Toward this goal, a \textbf{lightweight web server} could be implemented on each node, adding
support for REST APIs and access to historical metric data along with a graphical user interface for easier \dcamp
system management.

The current \dcamp protocols leave much to be desired when it comes to secure communication and operation. A more secure
implementation would include a form of \textbf{salted pass phrases} with every control message or even encrypt all
messages sent from one node to another.

One of the possible pain points with \dcamp is the control given to the system administrator through group
specifications. Specifically, administrators are tasked not only with identifying which nodes to include in the system,
but also how those nodes are placed into the distributed topology. Instead of this manual configuration,
\textbf{automatic grouping} of nodes may be implemented based on network locality, metric configuration and sample
periods, or even a tunable such as preference of network vs. CPU/memory overhead. The administrator would be left with
the task of defining which metrics a given node should collect and \dcamp would best select where the nodes sit in the
hierarchy, how many children nodes a single parent manages, etc.

\subsubsection{Fault Tolerance}

The fault tolerance of \dcamp could be improved by implementing these features which were considered out-of-scope for
the original project.

\dcamp does not support any fault tolerance for network failures---it only attempts to recover from node failures. It is
assumed that if (part of) the network goes down, the lack of data from that subnet will suffice. Specifically, \dcamp
cannot currently tolerate a \textbf{split-brain syndrome} in which the network has been partitioned and entire subsets
of the system cannot communicate with each other. It may be enhanced to recover from such network partitions, though.

The system time among multiple nodes in the distributed system may vary significantly. \dcamp is not meant to be a
high-resolution system with respect to the \textbf{ordering of performance data occurrences}. It is assumed that NTP
provides sufficient time synchronization across all nodes in the system OR the precise ordering of performance events in
the system is not required.

To further increase fault tolerance of the topology, \dcamp should be able to \textbf{operate without a \textit{Root}}
node. That is, the Management service should not be continuously needed for the system to operate. Essentially this
comes down to all top-level \textit{Collector} nodes being potential endpoints for end-user control, at which point it
momentarily acts as a Root, sending out configuration updates.

Lastly, as described in Chapter \ref{implementation}, \dcamp could become more resilient to software failures by running
\textit{Base} nodes within a \textbf{self-restarting executable}. If the process crashes for any reason, it would
automatically be restarted and join back into the network.

\subsubsection{Improve Performance and Scalability}

With several places for improvement, increasing the efficiency and performance of \dcampns's own implementation could
make really large systems feasible.

The current implementation of each ZeroMQ protocol heavily relies on a common polling pattern. Not only does this waste
thread resources waiting on socket connections, but the code becomes hard to maintain as well. An alternate solution to
this polling is event-driven I/O. ZeroMQ supports this alternate messaging pattern via Facebook's \textbf{Tornado
IOLoop}\cite{tornado}\cite{ioloop} and libev via \textbf{gevent}\cite{gevent}\cite{libev}.

With IOLoop, it may be possible to use a single IO loop, hosted by the \textit{Base} node, shared among all the active
services. This reduces the number of idle threads per node, freeing valuable operating system resources and reducing
\dcampns's processing overhead.

Although \dcamp only uses classic TCP protocols for all communication, ZeroMQ does support \textbf{multicast network
protocols}. Using multicast judiciously within \dcamp could greatly reduce configuration costs and network traffic, for
example in the \hyperref[proto_topo]{Topology Protocols}. For \dcamp systems spanning multiple subnets, the use of
multicast would require special network configurations or special ZeroMQ gateways for passing messages from one subnet
to the next.

\textbf{Multiple-level branches} are not supported in the current implementation. That is, all \textit{Collector} nodes
have the \textit{Root} node as their parent and only have \textit{Metric} nodes as their children. Extending support for
multiple levels of \textit{Collectors} would allow large group configurations to be automatically split into multiple
(identically configured) branches for improved scalability

Compiling the various critical paths within \dcampns, such as the metric sampling code in the Sensor service, using
\textbf{Pyrex}\cite{pyrex} or \textbf{Cython}\cite{cython} may boost performance and lower the cost of metric collection
such that faster sample periods can be used without issue.

Due to Python's Global Interpreter Lock\cite{py-threads}, there are limitations to the parallel execution of threads on
an SMP system. While \dcampns's use of threads is heavily I/O-bound, some gains may also be found by using full-fledged
\textbf{processes instead of threads}.

While not a huge cost, \dcamp currently requires two nodes to execute alongside each other on a system which hosts a
\textit{Collector}. An improvement would be to provide full support for \textbf{metric sampling directly within the
\textit{Collector} role}.

\subsubsection{Metric Extensions}
\label{metric_extensions}

Only a small subset of metrics were implemented in \dcamp as a proof of concept. The rest of the full set listed in the
\hyperref[dcamp_metrics]{\dcamp Metrics} section are left as future work.

Beyond the list of statically defined metrics, \textbf{user-defined metrics} would expand the performance monitoring
infinitely. This could be implemented as a Python module integrated into the distributed system being monitored or
through a plug-in system built into \dcamp itself.

Additionally, \dcamp could support additional data types such as \textbf{histograms} and \textbf{variable length
strings} or even more fine grained control over when metrics are sampled. For example, metrics could be
\textbf{collected on demand}, driven by user requests via the Management service, or collected at a special ``once''
sample period so data is sent to the \textit{Root} node only at start.

There are also two features which can be implemented to improve collection and reporting efficiency. First, a more
compact data message format could be used to \textbf{combine multiple data samples into a single message}, e.g. for
aggregation purposes or representing entire branches in the topology. This would improve network efficiency as fewer
packets would require routing and data could be more effectively compressed. Second, metrics could be sampled regularly
but \textbf{reported randomly} within the period in order to distribute arrival of data from child nodes and not
overload the Aggregation service.

Lastly, \dcamp could be extended to support some hardware performance counters, bringing it more in-line with hybrid
performance frameworks. In particular, it would be interesting to add support for Graphical Processing Unit metrics such
as those available via the NVIDIA Management Library\cite{nvidiaML} which already has Python bindings support
\cite{py-nvidia}.
