\chapter{Design}
\label{design}

\dcamp is designed to be simple and only add complexity where needed. This allows for quick and easy, large scale
testing, for example. Additionally, in order to be transparent, minimizing network traffic was an important concern.

To this end, \dcamp configuration and management protocols are designed to be human-readable and verbose; this makes
them easy to debug and modify as needed. The data protocol, while also human-readable in its current form, is terse with
respect to the number of required messages and can be easily modified to use a more compact encoding scheme.

\section{Architecture}

\dcamp is designed as a semi-centralized, hierarchical peer-to-peer system utilizing the pipe-flow architecture
pattern\cite{needed} in which leaf (\textit{Metric}) nodes of the hierarchy collect data, filter out extraneous data,
and send it up the pipe to a \textit{Collector} node which subsequently filters out more data and sends it up to another
\textit{Collector} or \textit{Root} node. This architecture is efficient in that unwanted data is discarded earlier in
the data path, reducing transport and processing costs.

\section{Requirements}

As with any software engineering project, it is vital to have clearly stated requirements. \dcamp is no different. Below
are the list of functional and non-functional requirements which guide its design and implementation.

\subsection{Functional}

\begin{enumerate}

\item Configuration and Management
      \begin{itemize}
      \item An interface to instantiate and administer the system MUST be provided.
      \item Topology coordination MUST be handled automatically.
      \item An interface for configuring metric collections MUST be provided.
      \end{itemize}

\item Metric Collection
      \begin{itemize}
      \item An API for stateful, aggregate metrics on top of CAMP must be provided.
      \item Filters and thresholds SHOULD be configurable at any level in the collection topology.
      \item Aggregation of metrics across nodes MUST be supported.
      \item Performance data SHOULD be written to log files on each node.
      \end{itemize}

\item Fault Tolerance
      \begin{itemize}
      \item The topology MUST sustain brief network disconnectivity of any node.
      \item The topology MUST handle entrance/exit of any node(s) in the system.
            \begin{itemize}
            \item \textit{Metric} nodes MUST be allowed to enter/exit topology at any time.
            \end{itemize}
      \item \textit{Root}/\textit{Collector} nodes (i.e. parents) MUST failover in case of extended disconnectivity.
            \begin{itemize}
            \item Loss of previously collected data SHOULD be minimized during failover.
            \end{itemize}
      \item Management node MAY be allowed to enter/exit topology at any time.
      \end{itemize}

\end{enumerate}

\subsection{Non-Functional}

\begin{itemize}

\item \textbf{Transparency:} \dcamp SHOULD introduce negligible performance impact on \textit{Metric} nodes.
\item \textbf{Accuracy:} \dcamp MUST accurately report performance of \textit{Metric} nodes (individual and aggregated).
\item \textbf{Scalability:} \dcamp SHOULD maintain its transparency and accuracy as it scales (i.e. the number of
      \textit{Metric} nodes increases).

\end{itemize}

\section{\dcamp Roles and Services}
\label{roles_and_services}

The \dcamp distributed system is comprised of one or more nodes, each executing a role. The role is essentially a named
grouping of a specific, known set of functionality or service. Roles have little to no actual run-time logic but simply
act as containers for the services; services manage ZeroMQ sockets, communicating with other services/nodes, and do the
real work of \dcamp.

\subsection{Services}

Each \dcamp service has a specific purpose, but its scope can vary depending on the node's level in the \dcamp topology.

For example, the Configuration service has a specific purpose of replicating the \dcamp configuration from the
\textit{Root} to every node in the system via three distinct scopes: root, branch, and leaf. As part of the
\textit{Root} node, the Configuration service acts as a master copy of the configuration and publishes new values as
needed. As part of a \textit{Collector} node, the Configuration service stores every update from the \textit{Root} (for
possible use if the \textit{Root} dies) but no other changes are allowed to be written. Lastly as part of a
\textit{Metric} node, only configuration updates relevant to the node are stored by the Configuration service.

\begin{itemize}

\item \textbf{Node}---rudimentary \dcamp functionality; handles topology communication, heartbeat monitoring, and failure
      recovery.
\item \textbf{Sensor}---local performance metric gathering; essentially the \dcamp layer on top of the OS and hardware
      performance APIs (accessed via CAMP).
\item \textbf{Filter}---performance metric filtering; provides throttling and thresholding of metrics.
\item \textbf{Aggregation}--—performance metric aggregation; provides collection of and calculation on metrics from
      multiple Sensor and/or Aggregation services.
\item \textbf{Management}--—primary entry-point for end-user control of \dcamp distributed system; this is the \dcamp
      instrument panel, providing basic administration functions (e.g. start, stop, etc.).
\item \textbf{Configuration}--—complete or partial configuration replication; provides topology and configuration
      distribution.

\end{itemize}

\subsection{Roles}

The \textit{Base} role must be running on each node for it to be part of the \dcamp distributed system. In this
document, a ``\textit{Base} node'' is defined as a \dcamp node which has not yet been configured, i.e. it has not joined
a running \dcamp system. All other roles are launched from within the \textit{Base} role; see Section
\ref{threading_model} for more details.

The \textit{Metric} role runs on the nodes from which performance metrics should be collected. The \textit{Collector}
role acts as an aggregation point in the system, combining performance data from multiple \textit{Metric} (and
\textit{Collector}) nodes and providing additional aggregated performance metrics.

There is only one \textit{Root} role active in the system; it acts as the master copy of the \dcamp configuration and
sole user-interface point. The \textit{Root} role is not strictly attached to any given node in the system. Rather, the
\textit{Root} role may dynamically move to any first-level \textit{Collector} node if the current \textit{Root} node
fails.

Depending on the use case and desired system performance, an administrator may choose to split roles across multiple
nodes or collapse them onto a single node. For example, a single node may act as \textit{Metric}, \textit{Collector},
and \textit{Root} for smaller systems while larger systems would employ dedicated \textit{Collector} nodes. The \dcamp
\hyperref[configuration]{Configuration} syntax easily provides this flexibility to the system administrator.

Table \ref{tab:role_to_services} lists the roles which can be executed by a \dcamp node and the services which they
implement.

\begin{table}
\begin{tabular}{l l}

\hline
\textbf{Role} & \textbf{Service(s)} \\
\hline

Root & Management, Aggregation, Filter, Configuration (Full) \\

Collector & Aggregation, Filter, Configuration (Full) \\

Metric & Sensor, Filter, Configuration (Partial) \\

Base & Node \\

\end{tabular}
\caption{Role to Service Mappings}
\label{tab:role_to_services}
\end{table}

\section{Fault Tolerance}

In order to maintain a healthy distributed topology, \dcamp quickly and efficiently recovers from node failures using
simple fault tolerance rules.

\subsection{Heartbeating (Detecting Disconnections)}

\dcamp detects node failures or disconnections via a lack of messages, e.g. missed X consecutive messages or no messages
received after D seconds. It is important to note that

\begin{itemize}
\item \textit{Metric} nodes MUST detect when their parent (\textit{Collector}) node disconnects.
      (\hyperref[algor_promo]{Promotion Algorithm})
\item The \textit{Root} node MAY detect when a \textit{Collector} node disconnects. (\hyperref[algor_promo]{Promotion
      Algorithm})
\item \textit{Collector} nodes MUST detect when the \textit{Root} node disconnects. (\hyperref[algor_elect]{Election
      Algorithm})
\item The \textit{Root} node MUST detect when a \textit{Metric} node rejoins the system.
      (\hyperref[algor_remind]{Reminder Algorithm})
\end{itemize}

LATER

\begin{itemize}
\item should the configuration TTL be used somehow?
      \begin{itemize}
      \item alternatively, this could be based on configuration entry TTL expiration.
      \item collector-to-collector heartbeating? not needed, since these TTLs will come from the root (via the data protocol)
      \item ttl used to know which first-level collectors are root-eligible?
      \end{itemize}
\item heartbeats are sent from first-level collectors to root or all collectors?
\item does a collector need to know when a metric node disconnects? no.
\end{itemize}

\subsection{Reminder Algorithm (Metric Node Recovery)}
\label{algor_remind}

Metric nodes can leave and enter the \dcamp system at any time. When they rejoin, they should be placed back into the
same location within the topology so as to maintain as much consistency within the performance data as possible.

The crux of this algorithm is the group definitions within the \dcamp configuration: nodes are always defined to be
within a group, and the groups define the network topology. Essentially, this algorithm is incorporated into the
Topology protocol; no additional work is necessary.

\begin{enumerate}
\item Sensor node rejoins the system with \texttt{POLO} response to Root node's \texttt{MARCO} message.
\item Root node detects Metric node is already part of \dcamp system.
\item Root node (re)sends \texttt{ASSIGN} message to Metric node.
\end{enumerate}

\subsubsection{DETECTION}

Detecting when a Metric node disconnects is not necessary. Rather the Root node only needs to detect when a Metric node
rejoins the \dcamp system, comparing the Metric node's UUID to the UUID already saved in the topology.

\subsubsection{QUESTIONS}

\begin{itemize}
\item How does this account for new nodes which have been assigned to Collector node since this Metric node
      disconnected? Can the Collector node become overloaded, or is this already accounted for with the configuration
      groups?
\end{itemize}

\subsection{Promotion Algorithm (Collector Node Recovery)}
\label{algor_promo}

As with the Reminder Algorithm, this recovery relies heavily on the \hyperref[proto_topo]{Topology Management Protocol}.
When a Metric node, M, detects its Collector node, C, is down, 

\begin{enumerate}
\item M sends an \texttt{SOS} message to the Root node, R.
\item If R has received an \texttt{SOS} message from more than 1/3 of C's group, the algorithm proceeds as per below.
\end{enumerate}

When the Root node, R, detects one of the Collector nodes has disconnected,

\begin{enumerate}
\item R broadcasts a \texttt{STOP} message to all nodes within C's group and clears the groups configuration from the
      \dcamp system.
\item R then broadcasts a \texttt{MARCO} message and begins rebuilding the group topology via the Topology protocol.
\end{enumerate}

\subsubsection{DETECTION}

M will use the \hyperref[proto_config]{Configuration Replication Protocol} (\texttt{HUGZ}) to detect when C disconnects.
R will use the \hyperref[proto_data]{Data Flow Protocol} (\texttt{DATA(type='HUGZ')}) to detect when any of its
Collector nodes has disconnected. That is, if M or R receives no messages from C within D seconds, C is considered
disconnected.

\subsection{Election Algorithm (Root Node Recovery)}
\label{algor_elect}

This recovery algorithm is based on the bully algorithm\cite{needed}. Only Collector nodes participate in the election,
initiated when a Collector node, C, detects the Root node, R, is down.

\begin{enumerate}
\item C sends \texttt{WUTUP} message to all nodes whose UUID is higher than its own, expecting a \texttt{YO} message in
      response.
\item If C does not receive any \texttt{YO} messages,
      \begin{enumerate}
      \item C declares victory by sending \texttt{IWIN} message to all nodes, and
      \item C waits W seconds before transitioning to become the Root, allowing for another node to replace it as Root
            via a separate election.
      \end{enumerate}
\item If C receives a \texttt{YO} message,
      \begin{enumerate}
      \item C waits for W seconds to receive an \texttt{IWIN} message from another node whose UUID is higher than its
            own.
      \item If no \texttt{IWIN} message is received, C resends its \texttt{WUTUP} message and goes through the election
            process again.
      \end{enumerate}
\end{enumerate}

Additionally,

\begin{itemize}
\item If C receives a \texttt{WUTUP} message from a node whose UUID is lower than its own, C responds with a \texttt{YO}
      message and then starts its own election.
\item If C receives an \texttt{IWIN} message from a node whose UUID is lower than its own, C immediately begins a new
      election.
\end{itemize}

\subsubsection{DETECTION}

C will use the \hyperref[proto_config]{Configuration Replication Protocol} (\texttt{HUGZ}) to detect when R disconnects.
That is, if C receives no message from R within D seconds, R is considered disconnected.

\subsubsection{QUESTIONS}

\begin{itemize}
\item Is there a difference between starting a new election and resending the initial \texttt{WUTUP} message? Is there
      an optimization to be found by uniquely ID'ing each election?
\item Can this algorithm be optimized w.r.t. the number of messages sent?
\end{itemize}

\section{\dcamp Metrics}
\label{dcamp_metrics}

The crux of any DPF are the performance metrics to which it provides access. This section lists the set of performance
metrics available via \dcamp. Metrics marked with ``(\emph{dCAMP})'' are extensions added by the \dcamp project to the
basic CAMP metrics. These provide a performance view of multiple nodes in the distributed network and are collected by
the Aggregation service rather than the Sensor service.

\subsection{Global Metrics}
Global metrics measure overall CPU, process, thread, and memory usage of the system.
\begin{itemize}
\item Node CPU usage
\item Node free physical memory
\item Aggregate average CPU usage (\dcamp)
\item Aggregate free physical memory (\dcamp)
\end{itemize}

\subsection{Network I/O Metrics}
Network metrics measure utilization of a given network interface on the system.
\begin{itemize}
\item Total bytes sent on the given interface
\item Total packets sent on the given interface
\item Total bytes received on the given interface
\item Total packets received on the given interface
\item Aggregate bytes sent (\dcamp)
\item Aggregate packets sent (\dcamp)
\item Aggregate bytes received (\dcamp)
\item Aggregate packets received (\dcamp)
\end{itemize}

\subsection{Disk I/O Metrics}
Disk I/O metrics measure throughput of a given disk or partition on the system.
\begin{itemize}
\item Number of read operations on the given disk
\item Number of write operations on the given disk
\item Number of read operations on the given partition
\item Number of write operations on the given partition
\item Aggregate number of read operations (\dcamp)
\item Aggregate number of write operations (\dcamp)
\end{itemize}

\subsection{Per-process Metrics}
Per-process metrics measure CPU, memory, and thread usage for a single process on the system.
\begin{itemize}
\item Number of major and minor page faults
\item Process CPU utilization
\item Process user mode CPU utilization
\item Process privileged mode CPU utilization
\item Size of the process' working set in KB
\item Size of the used virtual address space in KB
\item Number of threads contained in the process
\end{itemize}

\subsection{Inquiry Metrics}
Inquiry metrics provide a mechanism for enumerating various properties of the system.
\begin{itemize}
\item Enumeration of the available disk partitions
\item Enumeration of the available physical disks
\item Enumeration of the valid inputs to the network functions
\item Number of CPUs in the system
\item ``process identifier'' for the given PID
\item ``process identifier'' for each running process launched from an executable of the given name
\end{itemize}

\section{Configuration}
\label{configuration}

A main feature of \dcamp is the configuration language which gives a system administrator a concise, powerful tool for
defining its performance monitoring behaviour. Two primary sets of parameters are needed in this configuration: the set
of nodes to include in \dcamp and the set of metrics those nodes will collect.

While it would be possible for \dcamp to be designed such that branches of the distributed hierarchy are automatically
formed based on dynamic inputs (e.g. node locality, performance metric configuration, balance of network vs CPU/memory
impact), the approach taken in \dcamp gives the system administrator control over this parameter.

Specifically, nodes are defined in groups within the \dcamp configuration, and each group is defined to collect one or
more distinct performance metrics. This gives the system administrator the freedom to collect varying metrics from each
branch of the topology and/or collect metrics at varying sample periods.

\subsection{Node Specification}

Nodes may be specified individually (as ZeroMQ addresses) or as groups (IP subnets). Additionally, nodes may be included
or excluded based on host name or IP Address matching. Name matching does a case-insensitive comparison of the node's
host name; left, right, or whole name matching can be specified. Address matching checks that the node's IP Address
falls within a given subnet (i.e. IP Address and mask length).

\begin{figure}[H]
\vspace{+10pt}
\begin{verbatim}
  node-spec        = address / node-group
  address          = host ":" port
  host             = name / ip-address
  node-group       = name 1*( address / ( subnet ":" port ) ) *filter
  subnet           = ip-address "/" mask-length
  filter           = [ "+" / "-" ] ( name-match / subnet-match )
  name-match       = [ ( "L" / "R" / "W" ) SP ] name
  subnet-match     = subnet
\end{verbatim}
\vspace{-20pt}
\caption{Configuration File - Node Specification}
\label{fig:config_file_node}
\end{figure}

\subsection{Sample Specification}

Performance metric samples are specified as:

\begin{enumerate}
\item the node(s) on which to sample the data,
\item the rate at which data should be sampled,
\item the threshold past which data should be reported, and lastly
\item the actual performance metric to be sampled.
\end{enumerate}

The report threshold can be specified as ``hold and report every N seconds'' or ``report when the metric value is
greater/less than X''. When ``hold'' is specified (via an \texttt{*}), all metric values sampled during the time limit
are sent. Otherwise, the \texttt{<} or \texttt{>} character indicates the metric should only be reported when its
calculated value is less-than or greater-than the given threshold value.

\begin{figure}[ht]
\vspace{+10pt}
\begin{verbatim}
  sample-spec      = 1*sample
  sample           = sample-rate [ report-threshold ] metric
  sample-rate      = seconds
  report-threshold = ( "*" seconds ) / ( ( "<" / ">" ) 1*DIGIT )
  metric           = ( global-metric / process-metric process-name )
  global-metric    = "CPU" / "MEMORY" / "DISK" / "NETWORK"
  process-metric   = "PROC_CPU" / "PROC_MEM" / "PROC_IO"
  seconds          = 1*DIGIT "s"
\end{verbatim}
\vspace{-20pt}
\caption{Configuration File - Sample Specification}
\label{fig:config_file_sample}
\end{figure}

\subsubsection{Accumulative Time-Based Filtering Pitfall}

Filtering can be thought of being done in one of two ways: accumulatively or discretely. Accumulative means only one
final value is reported for each time range (e.g. collect every second but report every minute, so sixty samples are
combined into a single value and then sent). Discrete means each constituent value is sent for each time range, but they
are ``held'' until the time limit is reached.

However, accumulation is not valuable for monotonically increasing values--it is the same as just sampling at the slower
frequency. Accumulation is only valuable for non-monotonically increasing values, but in that case, one should find the
raw, monotonically increasing values from which it is calculated and collect that metric instead.

