\chapter{Implementation}
\label{implementation}

\section{Requirements}

\subsection{Functional}

\begin{enumerate}

\item Configuration
      \begin{itemize}
      \item instantiation, administration
      \item topology coordination
      \item metric collection spec.
      \end{itemize}

\item Metric Collection
      \begin{itemize}
      \item \dcamp API on top of CAMP
      \item filters (at various levels), thresholds
      \item aggregation of metrics across nodes
      \item output to log file
      \end{itemize}

\item Fault Tolerance
      \begin{itemize}
      \item simple rules to handle failures
      \end{itemize}

\end{enumerate}

\subsection{Fault Tolerance}

\begin{enumerate}

\item Topology MUST sustain brief network disconnectivity of any node.
\item Sensor nodes MUST be allowed to enter/exit topology at any time.
\item Root/Collector nodes (i.e. parents) MUST failover in case of extended disconnectivity.
      \begin{itemize}
      \item Loss of previously collected data SHOULD be minimized during failover.
      \end{itemize}
\item Management node SHOULD be allowed to enter/exit topology at any time.

\end{enumerate}

\subsection{Testing}

\begin{itemize}

\item \textbf{Transparency:} \dcamp SHOULD introduce negligible performance impact on sensor nodes.
\item \textbf{Accuracy:} \dcamp MUST accurately report metrics/performance of sensor nodes (individual and aggregated).
\item \textbf{Scalability:} \dcamp SHOULD maintain its transparency and accuracy as it scales (i.e. the number of sensor
      nodes increases).
\item \textbf{Fault Tolerance:} \dcamp MUST successfully handle entrance/exit of any node(s) in the system.

\end{itemize}

\section{\dcamp Operation}

\subsection{Sequence of \dcamp Operation}
\label{operation_sequnce}

The following steps describe how the \dcamp system is turned on. The Base nodes (other than the node assigned to be the
Root) can be started at any time by using the \dcamp CLI, before or after the Root node is initialized. It is expected
these Base nodes are managed by a watchdog utility which automatically restarts the node if it exits for any reason.

\begin{figure}[H]
\vspace{+10pt}
\begin{lstlisting}[language=bash,frame=single,basicstyle=\footnotesize\ttfamily]
#!/usr/bin/env bash
while [ true ]
do
    dcamp base --address localhost:56789
done
\end{lstlisting}
\vspace{-10pt}
\caption{Sample Watchdog Script}
\label{fig:sample_watchdog}
\end{figure}

\begin{enumerate}

\item User promotes a Root node via the \dcamp CLI, specifying a configuration file and a Base node's address.
\item Root node connects to each Base node and begins the ``discover'' \hyperref[proto_topo]{Topology Protocol}.
\item Base nodes join the \dcamp system at any time, being assigned as Collector or Metric nodes in the topology.

\item \dcamp runs in a steady state, nodes entering or exiting the system at any time.

      \begin{itemize}
      \item Performance counters are sampled, filtered, reported, and logged by the Metric nodes at regular intervals
            according to the \hyperref[configuration]{\dcamp Configuration}.
      \item Performance counters received from child nodes are aggregated, filtered, reported, and logged by Collector
            nodes at regular intervals according to the \dcamp Configuration.
      \item Performance counters received from child nodes are aggregated and logged by Root node for later processing
            (e.g. graphing metrics during a test scenario or correlating statistics with a distributed event log).
      \end{itemize}

\item User stops \dcamp by using the \dcamp CLI command.
\item Root node begins the ``stop'' Topology Protocol.
\item Collector and Metric nodes exit the topology and revert to Base nodes.
\item Root node exits, reverting to Base node.

\end{enumerate}

\subsection{Threading Model}

As mentioned above as the first and third steps of \dcamp operation, a Base node can transform into one of the three
active \dcamp roles: Root, Collector, or Metric. This transformation is actually the Base role (via the Node service)
launching and managing another Role internally. This interaction is depicted in Figure
\ref{fig:node_role_service_image}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{node-role-service.pdf}
    \caption[Node, Role, Services Threading Model Diagram]
            {Node, Role, Services Threading Model Diagram: Thread boundaries are represented by dashed lines. Except for
	     the Node service's \texttt{SUB} and \texttt{REQ} sockets, all arrows represent \texttt{PAIR} socket
	     communication.}
    \label{fig:node_role_service_image}
\end{figure}

When a Base node is running, only the bottom two threads (the Base role and the Node service) are active. Once it
receives an assignment from the ``discover'' Topology Protocol or the \dcamp CLI, the Node service launches an
appropriate Role thread which, in turn, launches one or more Role-specific service threads.

All communication between the roles and services occurs across \texttt{PAIR} control sockets. There are also various
service-to-service communications which occur via \texttt{inproc} transport sockets (e.g. the internal
\hyperref[proto_data]{Data Flow Protocol}) and shared memory data structures (e.g. the Configuration service).

Also mentioned in section \ref{operation_sequnce} as the last two steps, each Role exits and, by doing so, reverts
itself back to a Base node. This is handled just like before, with the Node service receiving a \texttt{STOP} message
via the ``stop'' Topology Protocol and then notifying the internally running Role to shut down. The Role thread then
notifies its service threads, waits for them to finish, then exits.

\section{ZeroMQ Protocols}

For a quick background on ZeroMQ socket types and message patterns, please see Appendix \ref{zeromq_primer}.

\input{tex/proto_topo}
\input{tex/proto_conf}
\input{tex/proto_data}
\input{tex/proto_reco}

